{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a30b0f44",
   "metadata": {},
   "source": [
    "_Learning to build a simple data warehouse gives me practical insight into how data flows from raw sources to actionable insights. Even though full-scale architecture is the responsibility of data architects, understanding the basics of modelling, ETL, and schema design helps me collaborate better with engineers, solve data issues more effectively, and approach analytics with a true end-to-end perspective._\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e338f517",
   "metadata": {},
   "source": [
    "### **Step 1** | Scoping out the project\n",
    "\n",
    "---\n",
    "\n",
    "#### Objective\n",
    "\n",
    "Develop a modern data warehouse using SQL Server to consolidate sales data, enabling analytical reporting and informed decision-making.\n",
    "\n",
    "#### Specifications\n",
    "\n",
    "- **Data Sources**: Import data from two source systems (ERP and CRM) provided as CSV files.\n",
    "- **Data Quality**: Cleanse and resolve data quality issues prior to analysis.\n",
    "- **Integration**: Combine both sources into a single, user-friendly data model designed for analytical queries.\n",
    "- **Scope**: Focus on the latest dataset only; historization of data is not required.\n",
    "- **Documentation**: Provide clear documentation of the data model to support both business stakeholders and analytics teams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85f446",
   "metadata": {},
   "source": [
    "#### Medallion Architecture\n",
    "\n",
    "As per the course’s recommendation, we will be using the Medallion data management paradigm. This layered approach is particularly effective for building a modern data warehouse because it separates raw, cleansed, and curated datasets into clear stages—commonly referred to as Bronze, Silver, and Gold layers. By structuring the pipeline this way, we ensure that each layer has a distinct purpose: Bronze holds unaltered, raw source data; Silver refines and cleanses this data for reliability; and Gold aggregates it into business-ready tables optimised for reporting and analytics.\n",
    "\n",
    "Opting for Medallion provides several advantages. It improves data quality and trust by ensuring transformations are traceable and reproducible, as all data first lands in a raw state before being standardised and validated. It also enhances maintainability and scalability, allowing us to debug issues at the appropriate layer rather than across the entire warehouse. \n",
    "\n",
    "<br>\n",
    "\n",
    "![data architecture figure 1 image](assets/img/journal_fig1.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "| | Bronze Layer | Silver Layer | Gold Layer |\n",
    "| - | - | - | - |\n",
    "| **Definition** | Raw, unprocessed data as-is from sources | Clean and standardised data | Business-ready data | \n",
    "| **Objective** | Traceability & debugging | (Intermediate layer) Prepare data for analysis | Provide data to be consumed for reporting & analytics |\n",
    "| **Object Type** | Tables | Tables | Views |\n",
    "| **Load Method** | Full load (truncate & insert) | Full load (truncate & insert) | None |\n",
    "| **Data Transformation** | None (as-is) | Data cleaning, standardisation, normalisation, enrichment & derived columns | Data integration, aggregation, business logic & rules |\n",
    "| **Data Modeling** | None (as-is) | None (as-is) | Start schema, aggregated objects, flat tables |\n",
    "| **Target Audience** | Data engineers | Data engineers & analysts | Data analysts & business users |  \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4775880",
   "metadata": {},
   "source": [
    "#### Layers for Separation of Concerns (SoC)\n",
    "\n",
    "The above layers mean that we have separation of concerns (SoC) - an important principle where we take a complex system and break it down into independent parts, each focused on a specific responsibility or operation without overlapping with others. So for a data warehouse, SoC means breaking the architecture into independent layers—such as ingestion, transformation, storage, and consumption—so each layer handles its own responsibility without interfering with the others...\n",
    "\n",
    "<br>\n",
    "\n",
    "![data architecture figure 2 image](assets/img/journal_fig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1573535",
   "metadata": {},
   "source": [
    "#### Beginnings...\n",
    "\n",
    "Moving forward, this journal will document each stage of the data warehouse build with clear, structured notes. We will capture:\n",
    "\n",
    "1. **Data Source Overview** – Key details about each source, including its format, refresh frequency, volume, and method of access.\n",
    "2. **Data Quality and Validation** – How we identify and resolve issues such as missing data, duplicates, and inconsistencies, as well as checks between Bronze and Silver layers.\n",
    "3. **Medallion Layer Objectives** – The purpose and responsibilities of the Bronze, Silver, and Gold layers, along with how each supports data trust and reporting readiness.\n",
    "4. **Target Schema Design** – Sketches and notes on fact and dimension tables, including any decisions around star vs. snowflake schema design.\n",
    "5. **ETL / ELT Flow** – Steps for moving data between layers, whether through incremental or full loads, and how the process will be orchestrated.\n",
    "6. **Testing and Monitoring** – Plans for data validation, quality checks, and ongoing monitoring to ensure the reliability of the Gold outputs.\n",
    "7. **Versioning and Progress Log** – A running journal of decisions, challenges, lessons learned, and key milestones as the project evolves.\n",
    "\n",
    "#### The high-level goal\n",
    "\n",
    "![data architecture figure 3 image](assets/img/journal_fig3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf77ee",
   "metadata": {},
   "source": [
    "### **Step 2** | Establishing rules & naming conventions\n",
    "\n",
    "---\n",
    "\n",
    "#### Naming Conventions\n",
    "\n",
    "We need naming conventions to keep our data systems clear, consistent, and scalable. They let us understand datasets and pipelines at a glance, reduce errors, and make automation with tools like Airflow or dbt much easier. Consistent names help us collaborate better, onboard new team members faster, and simplify governance tasks like lineage and documentation. With predictable patterns in place, our Medallion or multi-source warehouse can grow without becoming messy or confusing.\n",
    "\n",
    "#### General Principles\n",
    "- **Naming Conventions**: Use snake_case, with lowercase letters and underscores (_) to separate words.\n",
    "- **Language**: Use English for all names.\n",
    "- **Avoid Reserved Words**: Do not use SQL reserved words as object names.\n",
    "\n",
    "#### Table Naming Conventions\n",
    "\n",
    "**Bronze Layer Rules**\n",
    "\n",
    "All names must start with the source system name, and table names must match their original names without renaming.\n",
    "- `[sourcesystem_entity]`\n",
    "    - `[sourcesystem]`: Name of the source system (e.g., crm, erp).\n",
    "    - `[entity]`: Exact table name from the source system.\n",
    "    - Example: `crm_customer_info` → Customer information from the CRM system.\n",
    "\n",
    "**Silver Layer Rules**\n",
    "\n",
    "In this scenario, we are not renaming items between Bronze and Silver. So the rules above will apply in Silver. \n",
    "\n",
    "**Gold Layer Rules**\n",
    "\n",
    "All names must use meaningful, business-aligned names for tables, starting with the category prefix.\n",
    "- `[category_entity]`\n",
    "    - `[category]`: Describes the role of the table, such as dim (dimension) or fact (fact table).\n",
    "    - `[entity]`: Descriptive name of the table, aligned with the business domain (e.g., customers, products, sales).\n",
    "    - Examples:\n",
    "        - `dim_customers` → Dimension table for customer data.\n",
    "        - `fact_sales` → Fact table containing sales transactions.\n",
    "\n",
    "**Glossary of Category Patterns**\n",
    "| Pattern | Meaning | Example(s) |\n",
    "| - | - | - |\n",
    "| `dim_` | Dimension table | `dim_customer`, `dim_product` | \n",
    "| `fact_` | Fact table | `fact_sales` | \n",
    "| `agg_` | Aggregated table | `agg_customer`, `agg_sales_monthly` | \n",
    "\n",
    "\n",
    "#### Column Naming Conventions\n",
    "\n",
    "**Surrogate Keys**\n",
    "\n",
    "- All primary keys in dimension tables must use the suffix `_key`.\n",
    "- `[table_name]_key`\n",
    "    - [table_name]: Refers to the name of the table or entity the key belongs to.\n",
    "    - `_key`: A suffix indicating that this column is a surrogate key.\n",
    "    - Example: `customer_key` → Surrogate key in the dim_customers table.\n",
    "\n",
    "**Technical Columns**\n",
    "\n",
    "- All technical columns must start with the prefix dwh_, followed by a descriptive name indicating the column's purpose.\n",
    "- `dwh_[column_name]`\n",
    "    - `dwh`: Prefix exclusively for system-generated metadata.\n",
    "    - `[column_name]`: Descriptive name indicating the column's purpose.\n",
    "    - Example: `dwh_load_date` → System-generated column used to store the date when the record was loaded.\n",
    "\n",
    "**Stored Procedure**\n",
    "\n",
    "All stored procedures used for loading data must follow the naming pattern: \n",
    "- `load_[layer]`.\n",
    "    - `[layer]`: Represents the layer being loaded, such as bronze, silver, or gold.\n",
    "    - Example:\n",
    "        - `load_bronze` → Stored procedure for loading data into the Bronze layer.\n",
    "        - `load_silver` → Stored procedure for loading data into the Silver layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584fe172",
   "metadata": {},
   "source": [
    "### **Step 3** | Ready the database & schema\n",
    "\n",
    "---\n",
    "\n",
    "To begin capturing our design into code - we need to create and select a database, followed by establishing the schemas (which reflect our layers).\n",
    "\n",
    "A key rule to note with any shared scripts - particularly high-level ones (see below), is to include a clear description for each, as well as warnings regarding its purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf7b8fe",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*\n",
    "=============================================================\n",
    "Create Database and Schemas\n",
    "=============================================================\n",
    "Script Purpose:\n",
    "    This script creates a new database named 'DataWarehouse' after checking if it already exists. \n",
    "    If the database exists, it is dropped and recreated. Additionally, the script sets up three schemas \n",
    "    within the database: 'bronze', 'silver', and 'gold'.\n",
    "\t\n",
    "W A R N I N G :\n",
    "    Running this script will drop the entire 'DataWarehouse' database if it exists. \n",
    "    All data in the database will be permanently deleted. Proceed with caution \n",
    "    and ensure you have proper backups before running this script.\n",
    "*/\n",
    "\n",
    "USE master;\n",
    "GO\n",
    "\n",
    "-- Drop and recreate the 'DataWarehouse' database\n",
    "IF EXISTS (SELECT 1 FROM sys.databases WHERE name = 'DataWarehouse')\n",
    "BEGIN\n",
    "    ALTER DATABASE DataWarehouse SET SINGLE_USER WITH ROLLBACK IMMEDIATE;\n",
    "    DROP DATABASE DataWarehouse;\n",
    "END;\n",
    "GO\n",
    "\n",
    "-- create the DWH database\n",
    "CREATE DATABASE DataWarehouse;\n",
    "GO\n",
    "\n",
    "USE DataWarehouse;\n",
    "GO\n",
    "\n",
    "-- create schemas \n",
    "CREATE SCHEMA bronze;\n",
    "GO\n",
    "\n",
    "CREATE SCHEMA silver;\n",
    "GO\n",
    "\n",
    "CREATE SCHEMA gold;\n",
    "GO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bd114b",
   "metadata": {},
   "source": [
    "### **Step 4** | Developing the Bronze Layer\n",
    "\n",
    "---\n",
    "\n",
    "For us to begin building the first layer of our warehouse's architecture, we like any discipline, need to carefully **understand the sources** and the context surrounding them. We need to meet with stakeholders, and gauge:\n",
    "\n",
    "**Business Context & Ownership**\n",
    "- Who owns the data?\n",
    "- What business process it supports?\n",
    "- Systems and data documentation\n",
    "- Data model and data catalog \n",
    "\n",
    "**Architecture & Technology Stack**\n",
    "- How is data stored? \n",
    "- What are the integration capabilities?\n",
    "\n",
    "**Extract & Load**\n",
    "- Incremental vs. full load?\n",
    "- Data scope & historical needs\n",
    "- What is the expected size of the extracts?\n",
    "- Are there any data volume limitations?\n",
    "- How to avoid impacting the source system's performance?\n",
    "- Authentication and authorisation \n",
    "\n",
    "#### Specification of the Bronze Layer\n",
    "\n",
    "The Bronze Layer will handle raw, un-processed data as-is from sources. The overall objective of this layer is traceability and debugging.\n",
    "\n",
    "We are doing a full-load (truncate and insert), producing tables.\n",
    "\n",
    "To get started, we will explore the data to identify the column names and data types (ie. **Data Profiling**)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "376fffbe",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "cst_id,cst_key,cst_firstname,cst_lastname,cst_marital_status,cst_gndr,cst_create_date\n",
    "11000,AW00011000, Jon,Yang ,M,M,2025-10-06\n",
    "11001,AW00011001,Eugene,Huang  ,S,M,2025-10-06\n",
    "11002,AW00011002,Ruben, Torres,M,M,2025-10-06\n",
    "11003,AW00011003,Christy,  Zhu,S,F,2025-10-06\n",
    "11004,AW00011004, Elizabeth,Johnson,S,F,2025-10-06\n",
    "11005,AW00011005,Julio,Ruiz,S,M,2025-10-06\n",
    "11006,AW00011006,Janet,Alvarez,S,F,2025-10-06"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a610beb5",
   "metadata": {},
   "source": [
    "_previewing 8 of 15000+ lines_\n",
    "\n",
    "Referencing the data source above, and the naming conventions we set for each layer, we create our DDL (Data Definition Language). We repeat this process for each table to a total of six. These will store the raw data from the two sources folders (each holding three CSV files respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6686f46",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- if this table exists, remove it and create a new one\n",
    "IF OBJECT_ID ('bronze.crm_prd_info', 'U')  IS NOT NULL\n",
    "    DROP TABLE bronze.crm_prd_info;\n",
    "-- table creation with the required columns\n",
    "CREATE TABLE bronze.crm_prd_info (\n",
    "    prd_id INT,\n",
    "    prd_key NVARCHAR(50),\n",
    "    prd_nm NVARCHAR(50),\n",
    "    prd_cost INT,\n",
    "    prd_line NVARCHAR(2),\n",
    "    prd_start_dt DATE,\n",
    "    prd_end_dt DATE\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23eb0a",
   "metadata": {},
   "source": [
    "Once complete, we have six tables in the Bronze layer schema, of which all clearly reference their source system. \n",
    "\n",
    "```\n",
    "└── Bronze Layer/                        # schema           \n",
    "    ├── bronze.crm_cust_info/            # table for CRM \n",
    "    ├── bronze.crm_prd_info/\n",
    "    ├── bronze.crm_sales_details/        \n",
    "    ├── bronze.erp_cust_az12/            # table for ERP   \n",
    "    ├── bronze.erp_loc_a101/        \n",
    "    └── bronze.erp_px_cat_gtv2/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadac893",
   "metadata": {},
   "source": [
    "Following the structure setup, we then load the data via a bulk, truncate and insert process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6f9a6",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Prep the table for first load by making sure it is empty, or if previously loaded, avoid a duplication error\n",
    "TRUNCATE TABLE bronze.crm_cust_info;\n",
    "\n",
    "-- load the data from file\n",
    "BULK INSERT bronze.crm_cust_info\n",
    "FROM 'C:\\Users\\Bagheera\\My Drive\\07 DataEng\\sql-data-warehouse-project\\datasets\\source_crm\\cust_info.csv'\n",
    "-- below is we where provide the specification for the upload\n",
    "WITH (\n",
    "    FIRSTROW = 2, -- skip header row\n",
    "    FIELDTERMINATOR = ',', -- specify the field delimiter\n",
    "    TABLOCK\n",
    ");\n",
    "\n",
    "-- check results\n",
    "SELECT * FROM bronze.crm_cust_info;\n",
    "\n",
    "-- check row count exc. the first row\n",
    "SELECT COUNT(*) FROM bronze.crm_cust_info;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9120f277",
   "metadata": {},
   "source": [
    "Once we have basic structure in place, we can repeat for each data source, adding extensive formatting, keywords including `PRINT`, `TRY`, `DECLARE` throughout the code as a stored procedure. Doing such provides users with critical feedback, particularly in both system status and maintainance/debugging. This heavily increases the lines of code, but as Baraa Salkini notes:\n",
    "> _'...this is what data engineering is all about, not just loading the data, but engineering the entire pipeline, measuring the speed, accounting for errors, to print and capture each step in the ETL process.'_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2135035",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR ALTER PROCEDURE dbo.load_bronze AS \n",
    "BEGIN\n",
    "    DECLARE @start_time DATETIME, @end_time DATETIME, @batch_start_time DATETIME, @batch_end_time DATETIME;\n",
    "    BEGIN TRY\n",
    "        SET @batch_start_time = GETDATE();\n",
    "        PRINT '====================================';\n",
    "        PRINT 'Loading data into bronze layer...';\n",
    "        PRINT '====================================';\n",
    "        PRINT '------------------------------------';\n",
    "        PRINT 'Loading CRM tables ...';\n",
    "        PRINT '------------------------------------';\n",
    "        SET @start_time = GETDATE()\n",
    "        PRINT '>> Truncating table: bronze.crm_cust_info';\n",
    "        -- Prep the table for first load by making sure it is empty, or if previously loaded, avoid a duplication error\n",
    "        TRUNCATE TABLE bronze.crm_cust_info;\n",
    "        -- load the data from file\n",
    "        BULK INSERT bronze.crm_cust_info\n",
    "        FROM 'C:\\Users\\Bagheera\\My Drive\\07 DataEng\\sql-data-warehouse-project\\datasets\\source_crm\\cust_info.csv'\n",
    "        -- below is we where provide the specification for the upload\n",
    "        WITH (\n",
    "            FIRSTROW = 2, -- skip header row\n",
    "            FIELDTERMINATOR = ',', -- specify the field delimiter\n",
    "            TABLOCK\n",
    "        );\n",
    "        SET @end_time = GETDATE()\n",
    "        PRINT '>> Load Duration: ' + CAST(DATEDIFF(second, @start_time, @end_time) AS NVARCHAR(10)) + ' seconds';\n",
    "\n",
    "        SET @start_time = GETDATE()\n",
    "        PRINT '>> Truncating table: bronze.crm_prd_info';\n",
    "        TRUNCATE TABLE bronze.crm_prd_info;\n",
    "        BULK INSERT bronze.crm_prd_info\n",
    "        FROM 'C:\\Users\\Bagheera\\My Drive\\07 DataEng\\sql-data-warehouse-project\\datasets\\source_crm\\prd_info.csv'\n",
    "        WITH (\n",
    "            FIRSTROW = 2, \n",
    "            FIELDTERMINATOR = ',', \n",
    "            TABLOCK\n",
    "        );\n",
    "        SET @end_time = GETDATE()\n",
    "        PRINT '>> Load Duration: ' + CAST(DATEDIFF(second, @start_time, @end_time) AS NVARCHAR(10)) + ' seconds';\n",
    "\n",
    "        SET @start_time = GETDATE()\n",
    "        PRINT '>> Truncating table: bronze.sales_details';\n",
    "        TRUNCATE TABLE bronze.crm_sales_details;\n",
    "        BULK INSERT bronze.crm_sales_details\n",
    "        FROM 'C:\\Users\\Bagheera\\My Drive\\07 DataEng\\sql-data-warehouse-project\\datasets\\source_crm\\sales_details.csv'\n",
    "        WITH (\n",
    "            FIRSTROW = 2,\n",
    "            FIELDTERMINATOR = ',', \n",
    "            TABLOCK\n",
    "        );\n",
    "        SET @end_time = GETDATE()\n",
    "        PRINT '>> Load Duration: ' + CAST(DATEDIFF(second, @start_time, @end_time) AS NVARCHAR(10)) + ' seconds';\n",
    "\n",
    "        PRINT '------------------------------------';\n",
    "        PRINT 'Loading ERP tables ...';\n",
    "        PRINT '------------------------------------';\n",
    "        SET @start_time = GETDATE()\n",
    "        PRINT '>> Truncating table: bronze.cust_az12';\n",
    "        TRUNCATE TABLE bronze.erp_cust_az12;\n",
    "        BULK INSERT bronze.erp_cust_az12\n",
    "        FROM 'C:\\Users\\Bagheera\\My Drive\\07 DataEng\\sql-data-warehouse-project\\datasets\\source_erp\\cust_az12.csv'\n",
    "        WITH (\n",
    "            FIRSTROW = 2, \n",
    "            FIELDTERMINATOR = ',', \n",
    "            TABLOCK\n",
    "        );\n",
    "        SET @end_time = GETDATE()\n",
    "        PRINT '>> Load Duration: ' + CAST(DATEDIFF(second, @start_time, @end_time) AS NVARCHAR(10)) + ' seconds';\n",
    "        SET @start_time = GETDATE()\n",
    "        PRINT '>> Truncating table: bronze.loc_101';\n",
    "        TRUNCATE TABLE bronze.erp_loc_a101;\n",
    "        BULK INSERT bronze.erp_loc_a101\n",
    "        FROM 'C:\\Users\\Bagheera\\My Drive\\07 DataEng\\sql-data-warehouse-project\\datasets\\source_erp\\loc_a101.csv'\n",
    "        WITH (\n",
    "            FIRSTROW = 2, \n",
    "            FIELDTERMINATOR = ',', \n",
    "            TABLOCK\n",
    "        );\n",
    "        SET @end_time = GETDATE()\n",
    "        PRINT '>> Load Duration: ' + CAST(DATEDIFF(second, @start_time, @end_time) AS NVARCHAR(10)) + ' seconds';\n",
    "        SET @start_time = GETDATE()\n",
    "        PRINT '>> Truncating table: px_cat_g1v2';\n",
    "        TRUNCATE TABLE bronze.erp_px_cat_g1v2;\n",
    "        BULK INSERT bronze.erp_px_cat_g1v2\n",
    "        FROM 'C:\\Users\\Bagheera\\My Drive\\07 DataEng\\sql-data-warehouse-project\\datasets\\source_erp\\px_cat_g1v2.csv'\n",
    "        WITH (\n",
    "            FIRSTROW = 2,\n",
    "            FIELDTERMINATOR = ',',\n",
    "            TABLOCK\n",
    "        );\n",
    "        SET @end_time = GETDATE()\n",
    "        PRINT '>> Load Duration: ' + CAST(DATEDIFF(second, @start_time, @end_time) AS NVARCHAR(10)) + ' seconds';\n",
    "        \n",
    "        SET @batch_end_time = GETDATE();\n",
    "        PRINT '====================================';\n",
    "        PRINT 'Data loaded into bronze layer successfully.';\n",
    "        PRINT 'Total Load Duration: ' + CAST(DATEDIFF(second, @batch_start_time, @batch_end_time) AS NVARCHAR(10)) + ' seconds';\n",
    "        PRINT '====================================';\n",
    "    END TRY\n",
    "    BEGIN CATCH\n",
    "        PRINT '====================================';\n",
    "        PRINT 'Error occurred while loading data into Bronze layer:';\n",
    "        PRINT 'Error Message ' + ERROR_MESSAGE();\n",
    "        PRINT 'Error Message ' + CAST(ERROR_NUMBER() AS NVARCHAR(10));     \n",
    "        PRINT 'Error Message ' + CAST(ERROR_STATE() AS NVARCHAR(10));  \n",
    "        PRINT '====================================';\n",
    "    END CATCH\n",
    "END\n",
    "GO\n",
    "\n",
    "EXEC dbo.load_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4611be",
   "metadata": {},
   "source": [
    "So that `EXEC dbo.load_bronze;` returns..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4279d3fb",
   "metadata": {},
   "source": [
    "```\n",
    "Started executing query at Line 1\n",
    "====================================\n",
    "Loading data into bronze layer...\n",
    "====================================\n",
    "------------------------------------\n",
    "Loading CRM tables ...\n",
    "------------------------------------\n",
    ">> Truncating table: bronze.crm_cust_info\n",
    "(18493 rows affected)\n",
    ">> Load Duration: 0 seconds\n",
    ">> Truncating table: bronze.crm_prd_info\n",
    "(397 rows affected)\n",
    ">> Load Duration: 0 seconds\n",
    ">> Truncating table: bronze.sales_details\n",
    "(60398 rows affected)\n",
    ">> Load Duration: 1 seconds\n",
    "------------------------------------\n",
    "Loading ERP tables ...\n",
    "------------------------------------\n",
    ">> Truncating table: bronze.cust_az12\n",
    "(18483 rows affected)\n",
    ">> Load Duration: 0 seconds\n",
    ">> Truncating table: bronze.loc_101\n",
    "(18484 rows affected)\n",
    ">> Load Duration: 0 seconds\n",
    ">> Truncating table: px_cat_g1v2\n",
    "(37 rows affected)\n",
    ">> Load Duration: 0 seconds\n",
    "====================================\n",
    "Data loaded into bronze layer successfully.\n",
    "Total Load Duration: 1 seconds\n",
    "====================================\n",
    "Total execution time: 00:00:00.294\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30e1290",
   "metadata": {},
   "source": [
    "### **Step 5** | Developing the Silver Layer\n",
    "\n",
    "---\n",
    "\n",
    "#### Capturing the Data Flow\n",
    "\n",
    "With our source data _entering the warehouse_, we need to keep track of where it is going. We create a data flow diagram to capture where the data comes from, where it ends up, and how it moves through each layer. While similar to a high-level architecture diagram, it focuses solely on the data itself. This helps us develop the data lineage—a critical concept for maintaining reliable data pipelines.\n",
    "\n",
    "<br>\n",
    "\n",
    "![data architecture figure 4 image](assets/img/journal_fig4.png)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70983266",
   "metadata": {},
   "source": [
    "#### Workflow for Silver Layer\n",
    "\n",
    "Similar to Bronze, where we needed to explore the context and requirements. With Silver we need to explore and understand the data itself, principally because we are transitioning from raw data, to clean, standardiised data. In order to achieve this, we follow these steps.\n",
    "\n",
    "1. Review the data in its Bronze layer state.\n",
    "2. Data cleaning\n",
    "    - Check data quality\n",
    "    - Write data transformations\n",
    "    - Insert into silver layer\n",
    "3. Perform data correctness checks\n",
    "    - If errors or concerns appear -> cycle back to previous steps\n",
    "4. Data documentation\n",
    "\n",
    "#### Reviewing the data & anticipating an integration model\n",
    "\n",
    "To quickly preview a table in _SQL Server_, we can right click on table in the connections directory and 'SELECT' each. This gives an understanding of how various tables and the wider eco-system can relate to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f270986",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT TOP 10 * FROM bronze.crm_prd_info;\n",
    "SELECT TOP 10 * FROM bronze.crm_sales_details;\n",
    "SELECT TOP 10 * FROM bronze.crm_cust_info;\n",
    "SELECT TOP 10 * FROM bronze.erp_cust_az12;\n",
    "SELECT TOP 10 * FROM bronze.erp_loc_a101;\n",
    "SELECT TOP 10 * FROM bronze.erp_px_cat_g1v2;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4f7a96",
   "metadata": {},
   "source": [
    "With this, we can visualise how the relations/joins will work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494bea04",
   "metadata": {},
   "source": [
    "\n",
    "![data architecture figure 5 image](assets/img/journal_fig5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc72beda",
   "metadata": {},
   "source": [
    "The DDL tables for silver are a quick job, they are essentially the bronze DDL tables with replace prefixes.\n",
    "\n",
    "`CREATE TABLE bronze.crm_cust_info ()` becomes `CREATE TABLE silver.crm_cust_info ()`\n",
    "\n",
    "However, in the silver layer, we will be introducing something very important for the long-term...\n",
    "\n",
    "#### Metadata Columns\n",
    "\n",
    "Metadata columns are extra columns added by data engineers that do no originate from the source data. They provide further information, often specific to it's place in the warehouse. Examples could include:\n",
    "\n",
    "- **create_date**: The record's load timestamp.\n",
    "- **update_date**: The record's last update timestamp.\n",
    "- **source_system**: The origin system of the record.\n",
    "- **file_location**: The file source of the record. \n",
    "\n",
    "If ingrediants are delivered to a restaurant's kitchen, metadata columns would include the pantry shelf they have been stored upon, the time period they can be stored before cooking is required, the delivery service that refridgerated and delivered them. Important contextual information that is required, but not expected in the source data's original state.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c35e4f",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- revised table prefixes for silver layer\n",
    "IF OBJECT_ID ('silver.crm_cust_info', 'U')  IS NOT NULL\n",
    "    DROP TABLE silver.crm_cust_info;\n",
    "CREATE TABLE silver.crm_cust_info (\n",
    "    cst_id INT,\n",
    "    cst_key NVARCHAR(50),\n",
    "    cst_firstname NVARCHAR(50),\n",
    "    cst_lastname NVARCHAR(50),\n",
    "    cst_material_status NVARCHAR(50),\n",
    "    cst_gndr NVARCHAR(50),\n",
    "    cst_create_date DATE,\n",
    "    -- metadata added below\n",
    "    dwh_create_date DATETIME2 DEFAULT GETDATE()\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ef736",
   "metadata": {},
   "source": [
    "#### Checking & Cleaning the Data\n",
    "\n",
    "Before cleaning any dataset, we need to inspect it identify inconsistencies, and spot potential issues like missing values or duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477589e",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Check for Nulls or Duplicates in Primary Key\n",
    "-- Expectation: No Result.\n",
    "\n",
    "SELECT cst_id, COUNT(*)\n",
    "FROM bronze.crm_cust_info\n",
    "GROUP BY cst_id\n",
    "HAVING COUNT(*) > 1 OR cst_id IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2235da",
   "metadata": {},
   "source": [
    "This returns a few issues:\n",
    "\n",
    "|   | cst_id | (No column name) |\n",
    "|-----|--------|------------------|\n",
    "| 1   | 29449  | 2                |\n",
    "| 2   | 29473  | 2                |\n",
    "| 3   | 29433  | 2                |\n",
    "| 4   | NULL   | 3                |\n",
    "| 5   | 29483  | 2                |\n",
    "| 6   | 29466  | 3                |\n",
    "\n",
    "So lets look at a cst_id, which would ideally be a single row..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d86cb9",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM bronze.crm_cust_info\n",
    "WHERE cst_id = 29466;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b705e6",
   "metadata": {},
   "source": [
    "| Row | cst_id | cst_key      | cst_firstname | cst_lastname | cst_material_status | cst_gndr | cst_create_date |\n",
    "|-----|--------|--------------|----------------|---------------|----------------------|----------|------------------|\n",
    "| 1   | 29466  | AW00029466   | NULL           | NULL          | NULL                 | NULL     | 2026-01-25       |\n",
    "| 2   | 29466  | AW00029466   | Lance          | Jimenez       | M                    | NULL     | 2026-01-26       |\n",
    "| 3   | 29466  | AW00029466   | Lance          | Jimenez       | M                    | M        | 2026-01-27       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152e5a6",
   "metadata": {},
   "source": [
    "We can see three duplicate rows with increasing levels of data completeness. This suggests the customer's ID data is being inserted as a new record each time, rather than updating and replacing the existing entry.\n",
    "\n",
    "In this scenario, we are opting for the most recent version. To do this, we'll utilise a window function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b7c98",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM (\n",
    "    SELECT *,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY cst_id \n",
    "            ORDER BY cst_create_date DESC\n",
    "        ) as flag_last\n",
    "    FROM bronze.crm_cust_info\n",
    ") temp\n",
    "WHERE flag_last = 1\n",
    "-- test the above\n",
    "AND cst_id = 29466;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b4b90e",
   "metadata": {},
   "source": [
    "| cst_id | cst_key     | cst_firstname | cst_lastname | cst_material_status | cst_gndr | cst_create_date | flag_last |\n",
    "|--------|-------------|----------------|---------------|----------------------|----------|------------------|-----------|\n",
    "| 29466  | AW00029466  | Lance          | Jimenez       | M                    | M        | 2026-01-27       | 1         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3618923c",
   "metadata": {},
   "source": [
    "We apply this and other methods (to each table) to clean the data as we insert it into the silver layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f81408",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "INSERT INTO silver.crm_cust_info (\n",
    "    cst_id,\n",
    "    cst_key,\n",
    "    cst_firstname,\n",
    "    cst_lastname,\n",
    "    cst_material_status,\n",
    "    cst_gndr,\n",
    "    cst_create_date\n",
    ")\n",
    "SELECT \n",
    "    cst_id,\n",
    "    cst_key,\n",
    "    -- Trim unwanted spaces in string values\n",
    "    TRIM(cst_firstname) AS cst_firstname,\n",
    "    TRIM(cst_lastname) AS cst_lastname,\n",
    "\n",
    "    -- Data standarisation and consistency\n",
    "    -- Capture cst_marital_status as user-friendly references\n",
    "    -- Including UPPER in case original values appear in lowercase\n",
    "    CASE WHEN UPPER(TRIM(cst_marital_status)) = 'S' THEN 'Single'\n",
    "         WHEN UPPER(TRIM(cst_marital_status)) = 'M' THEN 'Married'\n",
    "         ELSE 'Unknown'\n",
    "    END AS cst_marital_status,\n",
    "\n",
    "    -- Repeat above for cst_gndr\n",
    "    CASE WHEN UPPER(TRIM(cst_gndr)) = 'F' THEN 'Female'\n",
    "         WHEN UPPER(TRIM(cst_gndr)) = 'M' THEN 'Male'\n",
    "         ELSE 'Unknown'\n",
    "    END AS cst_gndr,\n",
    "    cst_create_date\n",
    "\n",
    "    -- Removing duplicates\n",
    "    -- Window function to select the most recent entry\n",
    "    FROM (\n",
    "        SELECT *,\n",
    "            ROW_NUMBER() OVER (\n",
    "                PARTITION BY cst_id \n",
    "                ORDER BY cst_create_date DESC\n",
    "            ) as flag_last\n",
    "        FROM bronze.crm_cust_info\n",
    "    ) temp\n",
    "    WHERE flag_last = 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6cd17f",
   "metadata": {},
   "source": [
    "Cleaning and inserting all the tables, we now have a Silver layer.\n",
    "\n",
    "<br>\n",
    "\n",
    "![data architecture figure 6 image](assets/img/journal_fig6.png)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146cd67a",
   "metadata": {},
   "source": [
    "### **Step 6** | Developing the Gold Layer\n",
    "\n",
    "---\n",
    "\n",
    "Whilst we have cleaned and standardised the data, it is not truly business-ready. To do this, we must dedicated time and effort into understanding the businesses processes and data requirements.\n",
    "\n",
    "We need to think about the data in terms of high-level business areas... and there are three: sales, product & customer.\n",
    "\n",
    "#### From loading to modeling\n",
    "\n",
    "The Gold layer does not involve separate loading phases — only transformation processes such as integration, aggregation, and applying business rules and logic.\n",
    "\n",
    "The data will be modelled using a star schema, with aggregated objects stored in flat tables. We will have a fact table: Sales (quantitative focus, primarily representing events), and two Dimensions tables: Customer & Product (more descriptive information, providing context).\n",
    "\n",
    "Data modeling involves taking raw (and often cleansed) data and structuring in a meaningful way. There are three high-level variations of a data model: \n",
    "\n",
    "- **Conceptual data model** – A high-level blueprint that defines the business entities and their relationships, focusing on what data is important without technical detail.\n",
    "    (aka _the big picture_)  \n",
    "\n",
    "- **Logical data model** – A detailed design that defines how the data is organised, including attributes, relationships, and keys, without yet considering physical storage.\n",
    "    (aka _the blue print_)  \n",
    "\n",
    "- **Physical data model** – The implementation-ready design that specifies exactly how the data will be stored in a specific database, including table structures, data types, indexes, and constraints.\n",
    "    (aka _the step-by-step manual_)\n",
    "\n",
    "A lot of tools handle the physical data model, it can be very time consuming, so many architects focus on a well executed logical data model.\n",
    "\n",
    "#### Choosing a Star Schema\n",
    "\n",
    "For our project, our data model will follow a star schema, with multiple dimension tables organised around a central fact table. Our dimensions are limited in number but relatively large in size, and will only be joined a small number of times in typical queries. A star schema makes sense as our Gold layer needs to prioritise speed for analytical use, alongside simplicity for end users and BI tools. This 'less'-normalised approach reduces join complexity, improves query performance, and delivers a business-friendly model optimised for reporting.\n",
    "\n",
    "<br>\n",
    "\n",
    "![data architecture figure 7 image](assets/img/journal_fig7.png)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ae74f",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- =============================================================================\n",
    "-- dimension table for customers\n",
    "-- =============================================================================\n",
    "\n",
    "IF OBJECT_ID('gold.dim_customers', 'V') IS NOT NULL\n",
    "    DROP VIEW gold.dim_customers;\n",
    "GO\n",
    "\n",
    "CREATE VIEW gold.dim_customers AS\n",
    "SELECT\n",
    "    ROW_NUMBER() OVER (ORDER BY cst_id) AS customer_key, -- Surrogate key\n",
    "    ci.cst_id                          AS customer_id,\n",
    "    ci.cst_key                         AS customer_number,\n",
    "    ci.cst_firstname                   AS first_name,\n",
    "    ci.cst_lastname                    AS last_name,\n",
    "    la.cntry                           AS country,\n",
    "    ci.cst_marital_status              AS marital_status,\n",
    "    CASE \n",
    "        WHEN ci.cst_gndr != 'n/a' THEN ci.cst_gndr -- CRM is the primary source for gender\n",
    "        ELSE COALESCE(ca.gen, 'n/a')  \t\t\t   -- Fallback to ERP data\n",
    "    END                                AS gender,\n",
    "    ca.bdate                           AS birthdate,\n",
    "    ci.cst_create_date                 AS create_date\n",
    "FROM silver.crm_cust_info ci\n",
    "LEFT JOIN silver.erp_cust_az12 ca\n",
    "    ON ci.cst_key = ca.cid\n",
    "LEFT JOIN silver.erp_loc_a101 la\n",
    "    ON ci.cst_key = la.cid;\n",
    "GO\n",
    "\n",
    "-- =============================================================================\n",
    "-- dimension table for products\n",
    "-- =============================================================================\n",
    "\n",
    "IF OBJECT_ID('gold.dim_products', 'V') IS NOT NULL\n",
    "    DROP VIEW gold.dim_products;\n",
    "GO\n",
    "\n",
    "CREATE VIEW gold.dim_products AS\n",
    "SELECT\n",
    "    ROW_NUMBER() OVER (ORDER BY pn.prd_start_dt, pn.prd_key) AS product_key, -- Surrogate key\n",
    "    pn.prd_id       AS product_id,\n",
    "    pn.prd_key      AS product_number,\n",
    "    pn.prd_nm       AS product_name,\n",
    "    pn.cat_id       AS category_id,\n",
    "    pc.cat          AS category,\n",
    "    pc.subcat       AS subcategory,\n",
    "    pc.maintenance  AS maintenance,\n",
    "    pn.prd_cost     AS cost,\n",
    "    pn.prd_line     AS product_line,\n",
    "    pn.prd_start_dt AS start_date\n",
    "FROM silver.crm_prd_info pn\n",
    "LEFT JOIN silver.erp_px_cat_g1v2 pc\n",
    "    ON pn.cat_id = pc.id\n",
    "WHERE pn.prd_end_dt IS NULL; -- Filter out all historical data\n",
    "GO\n",
    "\n",
    "-- =============================================================================\n",
    "-- fact table for sales\n",
    "-- =============================================================================\n",
    "\n",
    "IF OBJECT_ID('gold.fact_sales', 'V') IS NOT NULL\n",
    "    DROP VIEW gold.fact_sales;\n",
    "GO\n",
    "\n",
    "CREATE VIEW gold.fact_sales AS\n",
    "SELECT\n",
    "    sd.sls_ord_num  AS order_number,\n",
    "    pr.product_key  AS product_key,\n",
    "    cu.customer_key AS customer_key,\n",
    "    sd.sls_order_dt AS order_date,\n",
    "    sd.sls_ship_dt  AS shipping_date,\n",
    "    sd.sls_due_dt   AS due_date,\n",
    "    sd.sls_sales    AS sales_amount,\n",
    "    sd.sls_quantity AS quantity,\n",
    "    sd.sls_price    AS price\n",
    "FROM silver.crm_sales_details sd\n",
    "LEFT JOIN gold.dim_products pr\n",
    "    ON sd.sls_prd_key = pr.product_number\n",
    "LEFT JOIN gold.dim_customers cu\n",
    "    ON sd.sls_cust_id = cu.customer_id;\n",
    "GO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b434369e",
   "metadata": {},
   "source": [
    "#### The Value of a Data Catalog\n",
    "\n",
    "After creating the gold layer, a key piece of housekeeping is to include a data catalog.\n",
    "\n",
    "The value of a data catalog is that it turns a messy, hard-to-navigate data landscape into something clear, searchable, and trustworthy. It helps us find the right data quickly, understand what it means, and know whether we can rely on it. \n",
    "\n",
    "By documenting definitions and lineage, it builds confidence in the data and supports better decision-making. It also keeps everyone aligned, reduces wasted effort from duplicated work, and makes governance and compliance far easier to manage. In short, it saves time, reduces risk, and makes our data actually usable. \n",
    "\n",
    "An example of table description for this warehouse:\n",
    "\n",
    "**_1. gold.dim_customers_**\n",
    "\n",
    "**Purpose:** Stores customer details enriched with demographic and geographic data.\n",
    "\n",
    "| Column Name      | Data Type     | Description                                                                                   |\n",
    "|------------------|---------------|-----------------------------------------------------------------------------------------------|\n",
    "| customer_key     | INT           | Surrogate key uniquely identifying each customer record in the dimension table.               |\n",
    "| customer_id      | INT           | Unique numerical identifier assigned to each customer.                                        |\n",
    "| customer_number  | NVARCHAR(50)  | Alphanumeric identifier representing the customer, used for tracking and referencing.         |\n",
    "| first_name       | NVARCHAR(50)  | The customer's first name, as recorded in the system.                                         |\n",
    "| last_name        | NVARCHAR(50)  | The customer's last name or family name.                                                     |\n",
    "| country          | NVARCHAR(50)  | The country of residence for the customer (e.g., 'Australia').                               |\n",
    "| marital_status   | NVARCHAR(50)  | The marital status of the customer (e.g., 'Married', 'Single', 'Unknown').                              |\n",
    "| gender           | NVARCHAR(50)  | The gender of the customer (e.g., 'Male', 'Female', 'Unknown').                                  |\n",
    "| birthdate        | DATE          | The date of birth of the customer, formatted as YYYY-MM-DD (e.g., 1971-10-06).               |\n",
    "| create_date      | DATE          | The date and time when the customer record was created in the system|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f594d14",
   "metadata": {},
   "source": [
    "**And with that, we've built a very basic data warehouse in SQL.**\n",
    "\n",
    "<br>\n",
    "\n",
    "![data architecture figure 8 image](assets/img/journal_fig8.png)\n",
    "\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
